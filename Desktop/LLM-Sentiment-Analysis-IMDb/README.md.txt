Comparaison de l'Analyse des Sentiments avec des LLMs Ajust√©s

Auteurs: FATHI MOUNTASSIR 
Institution : Universit√© Hassan II-Casablanca, Facult√© des Sciences Ben M'sick  

 üìù Introduction

Ce projet explore l'utilisation des Large Language Models (LLMs) pour l'analyse des sentiments √† partir du dataset IMDb. Les mod√®les utilis√©s sont DistilBERT, DistilRoBERTa et GPT-2, ajust√©s (fine-tuning) pour classifier les critiques de films en sentiments positifs ou n√©gatifs.

Les LLMs, bien que polyvalents, n√©cessitent souvent un ajustement sp√©cifique pour des t√¢ches particuli√®res comme l'analyse de sentiments dans des domaines sp√©cifiques.

üéØ Objectif

L'objectif de ce projet est de comparer les performances des mod√®les DistilBERT, DistilRoBERTa et GPT-2 sur la classification des sentiments √† partir du dataset IMDb.

Nous √©valuons leurs performances en termes de :
- Pr√©cision,
- Temps d'entra√Ænement,
- Efficacit√© des ressources,

afin de d√©terminer le meilleur compromis pour des applications r√©elles.

üìä M√©thodologie

1Ô∏è‚É£ Chargement et Pr√©traitement du Dataset
- Utilisation du dataset IMDb contenant des critiques de films √©tiquet√©es en positif/n√©gatif.
- Pr√©traitement des donn√©es (nettoyage, suppression du bruit textuel, normalisation).

2Ô∏è‚É£ Tokenization et Pr√©paration des Donn√©es
- Tokenisation des textes avec DistilBERT tokenizer pour transformer les donn√©es textuelles en entr√©es exploitables.
- Encodage des labels (0 pour n√©gatif, 1 pour positif).

3Ô∏è‚É£ Fine-Tuning des Mod√®les
- Ajustement des mod√®les pr√©-entra√Æn√©s (DistilBERT, DistilRoBERTa, GPT-2).
- Optimisation avec **AdamW** et ajustement des hyperparam√®tres (taux d'apprentissage, taille du batch).

4Ô∏è‚É£ √âvaluation des Mod√®les
- Entra√Ænement sur 80% du dataset, avec 20% r√©serv√©s √† la validation.
- M√©triques utilis√©es : Accuracy, Training Los, Validation Loss.

üîç Analyse des R√©sultats
- GPT-2 atteint la meilleure pr√©cision (94.07%), mais son entra√Ænement est plus long et demande plus de ressources.
- DistilBERT offre une performance comparable (94.04%), avec une empreinte m√©moire plus l√©g√®re.
- DistilRoBERTa est le plus rapide √† entra√Æner, mais affiche une pr√©cision l√©g√®rement inf√©rieure (92%).

üõ†Ô∏è Installation

Pour ex√©cuter ce projet, suivez ces √©tapes :

```bash
# Cloner le d√©p√¥t Git
git clone https://github.com/Mountassir-Fathi/LLM-Sentiment-Analysis-IMDb.git
cd LLM-Sentiment-Analysis-IMDb
```